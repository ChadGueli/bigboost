{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost Your Boost \n",
    "\n",
    "### Making Your ML WorkFlow Bigger and Faster\n",
    "\n",
    "In this notebook, we will cover:\n",
    "1. **Zarr** and its `Array` object for efficient, parallel data storage and retrieval.\n",
    "2. Parallelizing computations with **Dask**.\n",
    "3. Combining **Dask** and **XGBoost** to enable parallel regression on big (for simplicity, medium) data.\n",
    "4. Leveraging **Dask** to distribute hyperparameter search.\n",
    "5. Saving an **XGBoost** model for deployment in production code.\n",
    "\n",
    "Prerequisite: A working understanding of the machine learning workflow, NumPy, and Scikit-Learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages and Starting Our Client\n",
    "\n",
    "The imports are fairly straightforward. However, note that `dask_ml` and `dask` are different packages and need to be installed separately.\n",
    "\n",
    "More interestingly, we instantiate a Dask `Client`. Our `Client` manages scheduling, and generally attempts to optimize the parallelization of our code. Here, we have implicitly opened our client on our computer's local cluster (cpu). To use a different cluster, such as a cloud-based, hpc, or Kubernetes cluster, be sure to check out the [docs](https://docs.dask.org/en/stable/deploying.html).\n",
    "\n",
    "Invoking `client` in a notebook prints information about the client, and gives us a link to the Dask dashboard. While some graphics in the dashboard are straightforward, it may help to [read](https://docs.dask.org/en/latest/dashboard.html) about everything being shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chadgueli/miniconda3/envs/dayone/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import dask_ml.model_selection as dcv # unused due to bug\n",
    "import sklearn.model_selection as sklcv # used due to bug\n",
    "import dask_ml.metrics as dmetrics\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "from math import pi\n",
    "\n",
    "client = dask.distributed.Client()\n",
    "# client # uncomment for client info\n",
    "\n",
    "# Writing a py script? Use this instead.\n",
    "# if __name__ == '__main__':\n",
    "#     with dask.distributed.Client() as client:\n",
    "#         your dask code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation and Storage Writing\n",
    "\n",
    "We work with a parallelized data creation routine, and store the data in a `.zarr` file to practice both of these techniques. An added bonus is that it is more realistic to load our data from a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage with Zarr\n",
    "\n",
    "To make the data persistent, we open our `data.zarr` file as a `DirectoryStore`. That is, we use the `DirectoryStore` to act as if we are collecting and storing data for later use.\n",
    "Zarr offers many other storage formats that allow us to succinctly load data from our storage type of choice. For more info about the storage options, see the [Zarr docs](https://zarr.readthedocs.io/en/stable/api/storage.html).\n",
    "\n",
    "A great thing about Zarr files is that we are able to create in-file subdirectories called groups. In this way, Zarr files are like an easily movable file system focused on memory-and-speed-efficient array storage. Here we group the data into train and test sets.\n",
    "\n",
    "Finally, we call the `close` method on `zstore` so that Dask can do parallel reads and writes.\n",
    "\n",
    "**Why Zarr?** Some may read the preceding and think it sounds quite similar to [HDF5](https://www.h5py.org). Zarr has a similar interface to HDF5, but allows parallel reads and writes on the same array, giving it an advantage over HDF5 in the 2020s.\n",
    "\n",
    "**A Note On Zarr** Only array data storage is possible, but Zarr supports all NumPy dtypes. So feel free to create a `U8` file and name it `char.zarr`(d). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zstore = zarr.DirectoryStore('data.zarr')\n",
    "zroot = zarr.group(store=zstore)\n",
    "train, test = zroot.create_groups('train', 'test')\n",
    "\n",
    "zstore.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Data Creation with Dask.Array\n",
    "\n",
    "We are generating the data from the challenging [“Friedman #1”](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html#sklearn.datasets.make_friedman1) regression problem, and adding some irrelevant features and noise for fun. Composing the sine function on two variables makes this challenge particularly difficult for tree based models because of the large number of nodes needed to represent the curves. Nevertheless, we will be using XGBoost later in this chance to exhibit the strength of this estimator, even when its sub-estimators are suboptimal.\n",
    "\n",
    "Notably, we set the `chunks` arg while initializing our arrays. Chunks are sub-arrays stored as `numpy.ndarray` objects that Dask moves around in memory, and compresses when not in use. Chunks allow Dask to parallelize computation on arrays, even if they are too big to fit in memory. Dask will choose a default chunk size, but it may not be optimal in all use cases. For example, Dask recommends chunks of size at least 100 Mb, but our chunks are only 15.26 Mb because we found this size to be less-problematic with XGBoost.\n",
    "\n",
    "**IMPORTANT** `chunks` is short for chunk size. We are providing a size, not the number of chunks. Since `None` is the second element of the tuple passed to `chunks`, there is no chunking across rows. Thus, `X` is 10,000,000 by 20 and has chunks of size 100,000 by 20. For more info on optimal chunking, see this [Dask tutorial](https://docs.dask.org/en/latest/array-chunks.html).\n",
    "\n",
    "That's it, when this code is run, Dask will parallelize this computation for us, but it will not compute it yet. This code is actually telling dask to generate a task graph, which is Dask's particular name for a DAG; like how an `ndarray` is a tensor. Most of the Dask api is lazy, so it only runs computations when necessary. While `X` will be sampled and `y` will be executed in the next block, if we want to compute these values now and bring them into memory, we can call `.compute`.\n",
    "\n",
    "A directed acyclic graph (DAG) is a representation of the program, where nodes are objects, and edges are actions that produce new objects. Also there are no cycles, so no computations may occur in-place. Dask uses the DAG model to schedule jobs and plan communication between workers. Calling `.visualize`, we can generate a picture of our DAG when the [graphviz](https://anaconda.org/anaconda/graphviz) package is installed. Since the graph for our data creation is massive, only the portion pertaining to the first chunk is included. Since the chunks don't interact, each chunk has the same DAG.\n",
    "\n",
    "**Why Dask?** Some might say that similar functionality is available in [PySpark](https://spark.apache.org/docs/latest/api/Python/index.html). Ultimately, PySpark is a module that interprets your code into another language, Scala, where it creates a DAG, optimizes and compiles the DAG, and then finally runs the program (Confused?). Dask on the other hand uses Python to create and optimize the DAG that then runs in Python. Neither option is great as both provide partial programming languages hidden in another language, or in PySpark's case two other languages. Optimally, we would use Julia which walks like Python, runs like C, and provides meta-programming that allows the DAG with the full functionality of the language. Alas, Python is quite popular right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 10_000_000\n",
    "N_FTS = 20\n",
    "N_TRN = 4*N_OBS//5\n",
    "dim1 = 100_000\n",
    "\n",
    "# We dynamically assign to cut down on code and make the eqn easier to read.\n",
    "X = da.random.uniform(size=(N_OBS, N_FTS), chunks=(dim1, None))\n",
    "for i in range(5):\n",
    "    exec(f\"X{i} = X[:, {i}]\")\n",
    "    \n",
    "y = 2*da.sin(pi*X0*X1) + (2*X2 - 1)**2 + 2*X3 + X4\n",
    "y = 5*y + 0.1*da.random.normal(size=N_OBS, chunks=dim1)\n",
    "#y.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Creation DAG on 500k Observations](chunkdag.png)\n",
    "\n",
    "With the file structure set up, we write to our four Zarr arrays with Dask's `to_zarr` function. Again, we previously closed `data.zarr` because Dask won't write to arrays in memory.\n",
    "Zarr also chunks its arrays; using chunks as units of compression allows Zarr to keep the file size small and retrieval fast. Each `zarr.Array` will have the same chunking as the corresponding `dask.Array`.\n",
    "\n",
    "`y` is computed when we call `to_zarr`.\n",
    "\n",
    "Excitingly, the `zarr.Array` is quite literally an array as it supports much of the NumPy functionality, just out-of-core. So in the previous code block, we could have written the random values immediately to the `*/X` components, and then transformed them into the `*/y` components. However, this would have been slower both due to the high cost of memory retrieval from disk and a lack of parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.to_zarr(X[:N_TRN, :], 'data.zarr', component='train/X',)\n",
    "da.to_zarr(X[N_TRN:, :], 'data.zarr', component='test/X')\n",
    "da.to_zarr(y[:N_TRN], 'data.zarr', component='train/y')\n",
    "da.to_zarr(y[N_TRN:], 'data.zarr', component='test/y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading and Model Fitting\n",
    "We load the data from our Zarr file, and create an XGBoost regressor to predict future values. Given time, I hope to add examples for how to clean data with Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading with Dask\n",
    "\n",
    "Dask effortlessly reads from Zarr files with the `from_zarr` function. As with writing, chunk size is preserved between arrays.\n",
    "\n",
    "If you don't use a standard file format with a Dask function for reading either through Dask [arrays](https://docs.dask.org/en/latest/array-api.html#create-and-store-arrays) or [data frames](https://docs.dask.org/en/latest/dataframe-api.html#create-dataframes), I recommend using Dask's [delayed](https://docs.dask.org/en/latest/delayed.html) api.\n",
    "Wrap the function to load your data with the `delayed` function, convert the data to an array format, then move it to a `dask.Array`. This will maximize the efficiency of your code relative to Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = da.from_zarr('data.zarr', component='train/X')\n",
    "X_test = da.from_zarr('data.zarr', component='test/X')\n",
    "y_train = da.from_zarr('data.zarr', component='train/y')\n",
    "y_test = da.from_zarr('data.zarr', component='test/y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting with XGBoost\n",
    "\n",
    "For those unfamiliar, XGBoost is a machine learning package that provides access to several forms of gradient boosted models. Fortunately, the Python package exposes the Scikit-Learn api so you only need to learn the [math](https://xgboost.readthedocs.io/en/stable/tutorials/model.html) and the [hyperparameters](https://xgboost.readthedocs.io/en/stable/parameter.html).\n",
    "\n",
    "Unfortunately, there are some issues with Dask and XGBoost's interoperability. In particular, XGBoost's Dask models (e.g. `DaskXGBRegressor`) don't work with Dask's model selection objects (e.g. `RandomizedSearchCV`). Luckily, I found the [bug](https://github.com/dask/dask-ml/issues/849) causing this issue, and hopefully it will be fixed in a future update. Again, we are working with Dask v2022.4.0 and Dask_ML v2022.1.22. If you want to do this now, feel free to fork my [repo](https://github.com/ChadGueli/dask-ml).\n",
    "\n",
    "As a result of this bug, there are currently two options.\n",
    "1. Use `DaskXGB*` with Scikit-Learn's `RandomizedSearchCV` or `GridSearchCV`.\n",
    "2. Use `XGB*` with Dask's `RandomizedSearchCV` or `GridSearchCV`.\n",
    "\n",
    "The first option is preferable when you have big data and a small parameter search, the second is preferable when you have small data and a large parameter search. Now, the probabilists among us might point out that with large data, we could subset and search on a medium portion of the data, then train our final model on a larger portion. But, operations researchers will point out that regularization is dependent upon data size, so regularizer values selected on a medium subset should be suboptimal on the big train set. There are ways around this but these are beyond the scope of this notebook.\n",
    "\n",
    "As a bonus, the first option does allow for some Dask involvement in the CV search. Since SciKit-Learn uses Joblib to parallelize its algorithms, sometimes sub-optimally, and Joblib offers a Dask backend, we can give Dask knowledge of the CV search, helping with speed. Also Joblib is a truly beautiful little package to make parallelization easy, and I highly recommend it when Dask just isn't quite right; e.g. OOM OLS.\n",
    "\n",
    "Note that we use the DART, or Drop-out Additive Regression Trees booster. At each training step, this booster sets the coefficient of any tree in the linear combination to 0 with probability `rate_drop`, more info can be found in Rashmi and Gilad-Bachrach ([2015](https://proceedings.mlr.press/v38/korlakaivinayak15.pdf)).\n",
    "\n",
    "In general, when trying to constrain a hyperparameter search, as we are doing here for simplicity, my philosophy is to take each aspect of the model and select my favorite hyperparameter for regularizing it, then tune those hyper-parameters. So here we are using a DART booster and tweaking `rate_drop` to regularize the ensemble, with `max_depth` to regularize individual trees, and `lambda` (L2 shrinkage) for regularizing the complexity. Given enough time it would be wise to explore combinations of `eta` (learning rate) and `rate_drop`, `max_depth` and `max_leaves`, or `lambda` and `alpha` (L1 shrinkage).\n",
    "\n",
    "I like to tune the hyper-parameters in `params` because I believe that they lead to more compact trees, and smaller models. While Tan and Le's EfficientNet ([2019](https://arxiv.org/abs/1905.11946), [2021](https://arxiv.org/abs/2104.00298)) is a convolutional neural network, I encourage everyone to consider making their models as efficient as possible. In the case of a boosted tree model, that means minimizing the sum of the depths over all of the tree's. Unlike `eta` which decreases the impact of successive trees, `rate_drop` uses randomness as the regularization. In theory, this randomness produces more robust estimators, leading to fewer trees. Similarly, `max_depth` directly minimizes the tree depth, and `lambda` hinders model growth. To further optimize efficiency, we could select our final model based on the product of the final RMSE and the sum of tree depths.\n",
    "\n",
    "For the sake of simplicity and ease of reproducibility, we will not really perform the search. Instead, we will set `n_iter=1` so that only one set of params is evaluated, and `n_estimators=10`, so that only 10 trees are added per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chadgueli/miniconda3/envs/dayone/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chadgueli/miniconda3/envs/dayone/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chadgueli/miniconda3/envs/dayone/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/chadgueli/miniconda3/envs/dayone/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[15:55:09] task [xgboost.dask]:tcp://127.0.0.1:56995 got new rank 0\n",
      "[15:55:09] task [xgboost.dask]:tcp://127.0.0.1:56993 got new rank 1\n",
      "[15:55:09] task [xgboost.dask]:tcp://127.0.0.1:56994 got new rank 2\n",
      "[15:56:49] task [xgboost.dask]:tcp://127.0.0.1:56994 got new rank 0\n",
      "[15:56:49] task [xgboost.dask]:tcp://127.0.0.1:56995 got new rank 1\n",
      "[15:56:49] task [xgboost.dask]:tcp://127.0.0.1:56993 got new rank 2\n",
      "[15:58:27] task [xgboost.dask]:tcp://127.0.0.1:56994 got new rank 0\n",
      "[15:58:27] task [xgboost.dask]:tcp://127.0.0.1:56995 got new rank 1\n",
      "[15:58:27] task [xgboost.dask]:tcp://127.0.0.1:56993 got new rank 2\n",
      "[15:58:27] task [xgboost.dask]:tcp://127.0.0.1:56990 got new rank 3\n",
      "[16:00:06] task [xgboost.dask]:tcp://127.0.0.1:56995 got new rank 0\n",
      "[16:00:06] task [xgboost.dask]:tcp://127.0.0.1:56994 got new rank 1\n",
      "[16:00:06] task [xgboost.dask]:tcp://127.0.0.1:56993 got new rank 2\n",
      "[16:00:06] task [xgboost.dask]:tcp://127.0.0.1:56990 got new rank 3\n",
      "[16:02:08] task [xgboost.dask]:tcp://127.0.0.1:56993 got new rank 0\n",
      "[16:02:08] task [xgboost.dask]:tcp://127.0.0.1:56994 got new rank 1\n",
      "[16:02:08] task [xgboost.dask]:tcp://127.0.0.1:56995 got new rank 2\n",
      "[16:02:08] task [xgboost.dask]:tcp://127.0.0.1:56990 got new rank 3\n",
      "[16:04:09] task [xgboost.dask]:tcp://127.0.0.1:56990 got new rank 0\n",
      "[16:04:09] task [xgboost.dask]:tcp://127.0.0.1:56994 got new rank 1\n",
      "[16:04:09] task [xgboost.dask]:tcp://127.0.0.1:56995 got new rank 2\n",
      "[16:04:09] task [xgboost.dask]:tcp://127.0.0.1:56993 got new rank 3\n"
     ]
    }
   ],
   "source": [
    "lvl = da.arange(1, 4).compute() # NumPy array\n",
    "params = {\n",
    "    'max_depth': lvl+2,\n",
    "    'rate_drop': 0.015*lvl,\n",
    "    'lambda': 0.5*lvl\n",
    "}\n",
    "\n",
    "## Buggy Option: distribute regression and search\n",
    "# estim = xgb.dask.DaskXGBRegressor( booster='dart')\n",
    "# model = dcv.RandomizedSearchCV(estim, params)\n",
    "# model.fit(X_train, y_train, early_stopping_rounds=10)\n",
    "\n",
    "\n",
    "## Option 2: distribute search not regression\n",
    "# estim = xgb.XGBRegressor(booster='dart')\n",
    "# model = dcv.RandomizedSearchCV(estim, params)\n",
    "# model.fit(X_train, y_train, early_stopping_rounds=10)\n",
    "\n",
    "## Option 1: distribute regression not search\n",
    "estim = xgb.dask.DaskXGBRegressor(booster='dart', n_estimators=10)\n",
    "cv = sklcv.RandomizedSearchCV(estim, params, n_iter=1, n_jobs=2) \n",
    "with joblib.parallel_backend('dask'):         \n",
    "    cv.fit(X_train, y_train) # SKLearn's RandomizedSearchCV doesn't work with early_stopping_rounds\n",
    "\n",
    "model = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Saving Our Model\n",
    "\n",
    "Now we evaluate the best model on the test data, and save the model configuration with the `save_model` method; this stores all of the internal info for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5168143764151025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chadgueli/miniconda3/envs/dayone/lib/python3.9/site-packages/xgboost/sklearn.py:585: UserWarning: max_depth is not saved in Scikit-Learn meta.\n",
      "  warnings.warn(str(k) + ' is not saved in Scikit-Learn meta.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "rmse = lambda yt, yp: dmetrics.mean_squared_error(yt, yp, squared=False)\n",
    "print(rmse(y_test, model.predict(X_test)))\n",
    "\n",
    "model.save_model('smallmodel.txt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82bb9c6b9bcde6f633f268751e832d2939298f2736bce21e5a9bdc4929e5a34c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dayone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
